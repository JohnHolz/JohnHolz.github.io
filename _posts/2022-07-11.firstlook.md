<h1 style="text-align: center;">Primeira olhada no dataset</h1>

## Introdução

Temos um dataset grandinho, com mais de 450mb, então antes mesmo de começar podemos esperar $p$-valores distorcidos, graficos de distribuições dificieis de ver e muitos vazios.  
O mini report abaixo vai nós acompanhar antes de fazermos o modelo. A cada aparição veremos quantas linhas e colunas perdemos na limpeza. Esperamos que até o final mantenhamos o máximo para tentar inputar o máximo de informação nos modelos.

<table style="text-align: center">
<tr><th> n x p shape </th><th> Imbalanced Classes  </th></tr>
<tr><td>

| rows    | columns |
| ------- | ------- |
| 958.524 | 45      |

</td><td>

| Y   | Qtd.   |
| --- | ------ |
| N   | 936537 |
| Y   | 2066   |
| NA  | 19921  |

</td></tr> </table>

**Obs**: A seguir chamaremos rows de $n$ e colunas de $p$ por conta da literatura em cima, fica sempre a citação da bíblia [Introduction to Statistical Learning](https://www.statlearning.com/).

Vemos que temos linhas sobrando porem não temos pouquissimos asteroides potencialmente perigosos para testar nossos modelos. Então nosso foco é até o final mantermos esse 2066 asteroides.

### Table head

Nessa primeira visão do dataset vemos as primeiras linhas do dataset, logo entendemos muito sobre sua estrutura. Primeiro temos os dados referêntes ao nome do asteroide, depois temos as informações vitais anotadas, logo em seguida as medidas de incerteza, começadas por sigma.

    raw.head().transpose()

| col name       | 1 row           | 2 row           | 3 row          |
| -------------- | --------------- | --------------- | -------------- |
| id             | a0000001        | a0000002        | a0000003       |
| spkid          | 2000001         | 2000002         | 2000003        |
| full_name      | 1 Ceres         | 2 Pallas        | 3 Juno         |
| pdes           | 1               | 2               | 3              |
| name           | Ceres           | Pallas          | Juno           |
| prefix         | NaN             | NaN             | NaN            |
| neo            | N               | N               | N              |
| pha            | N               | N               | N              |
| H              | 3.4             | 4.2             | 5.33           |
| diameter       | 939.4           | 545.0           | 246.596        |
| albedo         | 0.09            | 0.101           | 0.214          |
| diameter_sigma | 0.2             | 18.0            | 10.594         |
| orbit_id       | JPL 47          | JPL 37          | JPL 112        |
| epoch          | 2458600.5       | 2459000.5       | 2459000.5      |
| epoch_mjd      | 58600           | 59000           | 59000          |
| epoch_cal      | 20190427.0      | 20200531.0      | 20200531.0     |
| equinox        | J2000           | J2000           | J2000          |
| e              | 0.076009        | 0.229972        | 0.256936       |
| a              | 2.769165        | 2.773841        | 2.668285       |
| q              | 2.558684        | 2.135935        | 1.982706       |
| i              | 10.594067       | 34.832932       | 12.991043      |
| om             | 80.305531       | 173.024741      | 169.851482     |
| w              | 73.597695       | 310.202392      | 248.066193     |
| ma             | 77.372098       | 144.975675      | 125.435355     |
| ad             | 2.979647        | 3.411748        | 3.353865       |
| n              | 0.213885        | 0.213345        | 0.226129       |
| tp             | 2458238.754129  | 2458320.962366  | 2458445.79219  |
| tp_cal         | 20180430.254129 | 20180721.462366 | 20181123.29219 |
| per            | 1683.145703     | 1687.410992     | 1592.013769    |
| per_y          | 4.608202        | 4.61988         | 4.358696       |
| moid           | 1.59478         | 1.23429         | 1.03429        |
| moid_ld        | 620.640533      | 480.348639      | 402.514639     |
| sigma_e        | 0.0             | 0.0             | 0.0            |
| sigmas...      | ...             | ...             | ...            |
| sigma_per      | 0.0             | 0.000004        | 0.000003       |
| class          | MBA             | MBA             | MBA            |
| rms            | 0.43301         | 0.35936         | 0.33848        |

## Porcentagem de valores diferentes e Porcentagem de dados em branco

Agora a primeira coisa que vamos tratar nossos problemas horizontais e verticais.

    Linhas e Colunas.

Queremos entender a variabilidade geral dos dados, procurando colunas unicas como ids e colunas com quantidade elevada de vazios para removermos do dataset, essas colunas não serão imputadas no modelo, e tambem pela alta quantidade de faltantes cortaremos da analise exploratória.

#### Quantidade de valores unicos nas colunas

Encontramos a quantidade de valores unicos pelo codigo a seguir, quantidade de unicos dividido pelo tamanho do dataset.

    (raw.nunique()/raw.shape[0]).sort_values()
    pha               0.000002
    sigma_per         0.294919
    sigma_tp          0.303848
    moid_ld           0.327901
    i                 0.999885
    tp_cal            0.999974
    a                 0.999984
    q                 0.999984
    per               0.999985
    n                 0.999990
    om                0.999994
    ma                0.999995
    tp                0.999995
    pdes              1.000000
    full_name         1.000000
    spkid             1.000000

##### Quantidade de Vazios

Da mesma maneira encontramos a quantidade de valores vazios, apenas trocamos o método de nunique pra isna e somamos, igual na [linguagem R](https://cran.r-project.org/), podemos simplesmente somar os True, ele será considerado como 1 e teremos o resultado esperado.

    (raw.isna().sum()/raw.shape[0]).sort_values()
    ma                0.000001
    per_y             0.000001
    rms               0.000002
    ad                0.000004
    per               0.000004
    moid_ld           0.000132
    H                 0.006534
    pha               0.020783
    moid              0.020783
    sigma_tp          0.020784
    sigma_n           0.020784
    diameter          0.857897
    diameter_sigma    0.858031
    albedo            0.859051
    name              0.976981
    prefix            0.999981

## Dropando

Agora finalmente a parte principal do artigo. Deletar tudo:

    Lidando com problemas horizontais e verticais...

1. Dropando rows onde variavel resposta em branco.
2. Removemos as colunas IDs/nome;
3. Dropando colunas com mais de 70% de vazios;

Agora o que nos resta é apresentado abaixo, ainda precisamos lidar com os pouquissimos dados faltantes no dataset.

#### Tabela de vazios pós remover rows com resposta em branco

    full_name         0.000000
    sigma_tp          0.000001
    sigma_n           0.000001
    sigma_e           0.000001
    per_y             0.000001
    ma                0.000001
    rms               0.000001
    ad                0.000004
    neo               0.000004
    per               0.000004
    sigma_ad          0.000005
    sigma_per         0.000005

Verificando manualmente vemos que são alguns dados, 7 linhas, em que quase todas colunas estão em branco e os asteroides são classificados como não há risco, então dropamos essas linhas, como já temos nosso dataset desbalanceado. O dataset final ficou da seguinte forma:

<table>
<tr><th> n x p shape </th><th> Imbalanced Classes  </th></tr>
<tr><td>

| n       | p   |
| ------- | --- |
| 238.603 | 39  |
| \*      |     |

</td><td>

| Y   | Qtd     |
| --- | ------- |
| N   | 936.537 |
| Y   | 2066    |

</td></tr> </table>

## Primeiras Notas

1. Notas:
   1. **Variavel resposta Y:PHA**
   2. **Variaveis não númericas: NEO(binaria) e CLASS**
   3. **Todas outras variaveis são númericas**
2. First Cleaning:
   1. **Dados com variavel resposta vazia dropados**
   2. **Colunas primeiras 6 colunas ids/nomes dropados**
   3. **Colunas com alta % de vazios dropados**

# Conclusão

Optivemos sucesso na nossa primeira olhada dos dados, a primeira limpada retirando o grosso do que não queremos. E na próxima etapa apenas explorarmos apenas os dados "uteis". Identificando bem as variáveis, já nos adiantamos com muitos possiveis problemas futuros, tendo em mente que mesmo perdendo colunas e rows hoje não perdemos praticamente nada de informação, e aumentaremos nosso $p$ dummificando a coluna class.

#### Próximo artigo: [Artigo 3](Artigo 3)
